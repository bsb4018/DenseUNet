{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6axgKBf_dR5k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6axgKBf_dR5k",
    "outputId": "cde63e52-be80-4699-8128-ce8e431d6de3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting git+https://github.com/tensorflow/docs\n",
      "  Cloning https://github.com/tensorflow/docs to /tmp/pip-req-build-u4sprovf\n",
      "  Running command git clone -q https://github.com/tensorflow/docs /tmp/pip-req-build-u4sprovf\n",
      "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (0.8.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (1.0.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (2.11.3)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (3.17.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (3.13)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<3.20,>=3.12.0->tensorflow-docs==0.0.0.dev0) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->tensorflow-docs==0.0.0.dev0) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "X2eeqdWzdFzQ",
   "metadata": {
    "id": "X2eeqdWzdFzQ"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba4e5c7",
   "metadata": {
    "id": "4ba4e5c7"
   },
   "outputs": [],
   "source": [
    "#Encoder and base of our network will be pretrained DenseNet121\n",
    "#Decoder will be an upsample blocks of size 512,256,128,64 that will trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39a329a1",
   "metadata": {
    "id": "39a329a1"
   },
   "outputs": [],
   "source": [
    "#Load the Dataset\n",
    "dataset, info = tfds.load('oxford_iiit_pet',with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "I17_vC4XnJiT",
   "metadata": {
    "id": "I17_vC4XnJiT"
   },
   "outputs": [],
   "source": [
    "#Load image and mask and normalize ,additionally do augmentation during training\n",
    "@tf.function\n",
    "def load_image(dataset_element, train=True):\n",
    "    input_image = tf.image.resize(dataset_element['image'], (256,256))\n",
    "    input_mask = tf.image.resize(dataset_element['segmentation_mask'], (256,256))\n",
    "    \n",
    "    if train and np.random.uniform() > 0.5:\n",
    "        input_image = tf.image.flip_left_right(input_image) #data augmentation during training\n",
    "    \n",
    "    input_image =  tf.cast(input_image, tf.float32) / 255.0 #Normalize image --> tf.cast converts the \n",
    "                                                            #image which is a tensor to float type\n",
    "    input_mask = input_mask - 1 #Normalize mask\n",
    "        \n",
    "    return input_image,input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "yKtBw51TnOg0",
   "metadata": {
    "id": "yKtBw51TnOg0"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "#Autotuning gives better performance tf.data builds a performance \n",
    "#model of the input pipeline and runs an optimization algorithm to \n",
    "#find a good allocation of its CPU budget across all parameters \n",
    "#specified as AUTOTUNE. While the input pipeline is running, \n",
    "#tf.data tracks the time spent in each operation, so that these times \n",
    "#can be fed into the optimization algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "qULUsY8LfM2U",
   "metadata": {
    "id": "qULUsY8LfM2U"
   },
   "outputs": [],
   "source": [
    "#Model Definition \n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "class DenseUnet(object):\n",
    "    def __init__(self, input_size=(256,256,3),output_channels=3):\n",
    "        self.pretrained_model = DenseNet121(input_shape=input_size,\n",
    "                                           include_top=False,\n",
    "                                           weights='imagenet')\n",
    "        \n",
    "        #The encoding part of the network which will be frozen\n",
    "        self.target_layers = [\n",
    "            'conv1/relu', #1st encoder block going down to 2nd\n",
    "            'conv2_block1_0_relu', #2nd encoder block going down to 3rd\n",
    "            'conv3_block1_0_relu', #3rd encoder block going down to 4th\n",
    "            'conv4_block1_0_relu', #4th encoder block going down to base\n",
    "            'conv5_block8_1_relu' #base\n",
    "        ]\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_channels = output_channels\n",
    "        \n",
    "        self.model = self._create_model()\n",
    "        loss = SparseCategoricalCrossentropy(from_logits=True)\n",
    "        self.model.compile(optimizer=RMSprop(),loss=loss,metrics=['accuracy'])\n",
    "\n",
    "        #_create_model() creates the model as defined\n",
    "        #self.target_layers will be used for transfer learning       \n",
    "        #output_channels is 3 , because each pixel can be categorized into\n",
    "        #one of three classes black white and grey \n",
    "        #Trainer->RMSprop Loss->SparseCategoricalCrossentropy\n",
    "    \n",
    "    #Define our decoder 1.e. upsample block\n",
    "    @staticmethod\n",
    "    def _upsample(filters, size, dropout=False):\n",
    "       \n",
    "        init = tf.random_normal_initializer(0.0, 0.03) #Kernel weight initializer\n",
    "        #Upsample block will be made of transposed convolutions\n",
    "        layers = Sequential()\n",
    "        layers.add(Conv2DTranspose(filters=filters,\n",
    "                              kernel_size=size,\n",
    "                              strides=2,\n",
    "                              padding='same',\n",
    "                              kernel_initializer=init,\n",
    "                              use_bias = False))\n",
    "        layers.add(BatchNormalization())\n",
    "        if dropout: layers.add(Dropout(rate=0.65))\n",
    "        layers.add(ReLU())\n",
    "    \n",
    "        return layers\n",
    "    \n",
    "    #create the model\n",
    "    def _create_model(self):\n",
    "        layers = []\n",
    "        for i in self.target_layers:\n",
    "            layers.append(self.pretrained_model.get_layer(i).output)\n",
    "            \n",
    "        down_stack = Model(inputs=self.pretrained_model.input, outputs=layers)\n",
    "        down_stack.trainable = False #don't train the encoder\n",
    "        \n",
    "        up_stack = []\n",
    "        for filters in (512, 256, 128, 64): #upsample to blocks of these sizes\n",
    "            up_block = self._upsample(filters, 4)\n",
    "            up_stack.append(up_block)\n",
    "            \n",
    "        #Upsampling from block of size 512 going up \n",
    "        #to block 256 to 128 to finally 64\n",
    "        \n",
    "        inputs = Input(shape=self.input_size)\n",
    "        x = inputs\n",
    "        \n",
    "        #Adding skip connection to enable flow of gradient\n",
    "        skip_layers = down_stack(x) #the output of encoder blocks\n",
    "        x = skip_layers[-1]         #taking the last output i.e. base output that will serve as the input to 1st upsampling layer  \n",
    "        skip_layers = reversed(skip_layers[:-1]) #reverse the encoder ouput(-1) indicates base layer not taken since we will concatenate \n",
    "                                                 #the last encoder output with the first upsampled layer\n",
    "         \n",
    "        for up, skip_connection in zip(up_stack,skip_layers):\n",
    "            x = up(x)                                  #upsample input to the defined upsample stack\n",
    "            x = Concatenate()([x, skip_connection])    #concatenate the encoder output\n",
    "        \n",
    "        #output of upsampled+concatenated goes to a transposed convolution\n",
    "        init = tf.random_normal_initializer(0.0, 0.03) #weight initialization for the kernel\n",
    "        output = Conv2DTranspose(\n",
    "            filters=self.output_channels,\n",
    "            kernel_size=3,\n",
    "            strides=2,\n",
    "            padding='same',\n",
    "            kernel_initializer=init)(x)\n",
    "            \n",
    "        return Model(inputs, outputs=output)\n",
    "    \n",
    "    #Plot training plots\n",
    "    @staticmethod\n",
    "    def _plot_model_history(model_history, metric, ylim=None):\n",
    "        plt.style.use('seaborn-darkgrid')\n",
    "        plotter = tfdocs.plots.HistoryPlotter()\n",
    "        plotter.plot({'Model': model_history}, metric=metric)\n",
    "        plt.title(f'{metric.upper()}')\n",
    "        if ylim is None:\n",
    "            plt.ylim([0, 1])\n",
    "        else:\n",
    "            plt.ylim(ylim)\n",
    "        plt.savefig(f'{metric}.png')\n",
    "        plt.close()\n",
    "        \n",
    "    def train(self, train_dataset, epochs, steps_per_epoch,\n",
    "              validation_dataset, validation_steps):\n",
    "        hist = self.model.fit(train_dataset,epochs=epochs,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              validation_steps=validation_steps,\n",
    "                              validation_data=validation_dataset)\n",
    "        \n",
    "        self._plot_model_history(hist, 'loss', [0.,2.0])\n",
    "        self._plot_model_history(hist, 'accuracy')\n",
    "        \n",
    "\n",
    "    #Create a mask from model prediction\n",
    "    @staticmethod\n",
    "    def _create_mask(prediction_mask):\n",
    "        prediction_mask = tf.argmax(prediction_mask,axis=-1)\n",
    "        prediction_mask = prediction_mask[...,tf.newaxis]\n",
    "        return prediction_mask[15]\n",
    "\n",
    "    #Open segmentation mask with opencv to later save and view  \n",
    "    @staticmethod\n",
    "    def _process_mask(mask):\n",
    "        mask = (mask.numpy() * 127.5).astype('uint8')\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "        return mask\n",
    "\n",
    "    #Save and Visualization iamage and masks\n",
    "    def _save_image_and_masks(self, image,ground_truth_mask,\n",
    "                              prediction_mask, image_id):\n",
    "        \n",
    "        image = (image.numpy() * 255.0).astype('uint8')\n",
    "        gt_mask = self._process_mask(ground_truth_mask)\n",
    "        pred_mask = self._process_mask(prediction_mask)\n",
    "        mosaic = np.hstack([image, gt_mask, pred_mask])\n",
    "        mosaic = cv2.cvtColor(mosaic, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(f'segimg_{image_id}.jpg', mosaic)\n",
    " \n",
    "    #save the predicted mask \n",
    "    def _save_predictions(self, dataset,sample_size=1):\n",
    "        for id, (image, mask) in enumerate(dataset.take(sample_size),\n",
    "                                           start=1):\n",
    "            pred_mask = self.model.predict(image)\n",
    "            pred_mask = self._create_mask(pred_mask)\n",
    "            image = image[15]\n",
    "            ground_truth_mask = mask[15]\n",
    "            self._save_image_and_masks(image,ground_truth_mask,\n",
    "                                       pred_mask,image_id=id)\n",
    "            \n",
    "    #Compute the accuracy\n",
    "    def evaluate(self, test_dataset, sample_size=5):\n",
    "        result = self.model.evaluate(test_dataset)\n",
    "        print(f'Accuracy: {result[1] * 100:.2f}%')\n",
    "        self._save_predictions(test_dataset, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e806ae8a",
   "metadata": {
    "id": "e806ae8a"
   },
   "outputs": [],
   "source": [
    "#Hperparameters\n",
    "TRAIN_SIZE = info.splits['train'].num_examples\n",
    "VALIDATION_SIZE = info.splits['test'].num_examples\n",
    "BATCH_SIZE = 64\n",
    "STEPS_PER_EPOCH = TRAIN_SIZE // BATCH_SIZE\n",
    "VALIDATION_SUBSPLITS = 5\n",
    "VALIDATION_STEPS = VALIDATION_SIZE // BATCH_SIZE\n",
    "VALIDATION_STEPS //= VALIDATION_SUBSPLITS\n",
    "BUFFER_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b77af07",
   "metadata": {
    "id": "1b77af07"
   },
   "outputs": [],
   "source": [
    "#Define the training and testing datasets\n",
    "train_dataset = (dataset['train'].map(load_image, num_parallel_calls=AUTOTUNE)\n",
    "                 .cache()\n",
    "                 .shuffle(BUFFER_SIZE)\n",
    "                 .batch(BATCH_SIZE)\n",
    "                 .repeat()\n",
    "                 .prefetch(buffer_size=AUTOTUNE))\n",
    "test_dataset = (dataset['test']\n",
    "                 .map(lambda d: load_image(d,\n",
    "                 train=False),\n",
    "                 num_parallel_calls=AUTOTUNE)\n",
    "                 .batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c1ad0d26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1ad0d26",
    "outputId": "7c3ca7e6-d9ae-4fbe-ba18-48e5bcdee06f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "57/57 [==============================] - 45s 694ms/step - loss: 0.4687 - accuracy: 0.8153 - val_loss: 1.0023 - val_accuracy: 0.6141\n",
      "Epoch 2/15\n",
      "57/57 [==============================] - 37s 658ms/step - loss: 0.3062 - accuracy: 0.8734 - val_loss: 0.8415 - val_accuracy: 0.6949\n",
      "Epoch 3/15\n",
      "57/57 [==============================] - 37s 653ms/step - loss: 0.2711 - accuracy: 0.8867 - val_loss: 0.4985 - val_accuracy: 0.7916\n",
      "Epoch 4/15\n",
      "57/57 [==============================] - 37s 655ms/step - loss: 0.2473 - accuracy: 0.8956 - val_loss: 0.3777 - val_accuracy: 0.8479\n",
      "Epoch 5/15\n",
      "57/57 [==============================] - 37s 654ms/step - loss: 0.2275 - accuracy: 0.9033 - val_loss: 0.3956 - val_accuracy: 0.8364\n",
      "Epoch 6/15\n",
      "57/57 [==============================] - 37s 655ms/step - loss: 0.2093 - accuracy: 0.9100 - val_loss: 0.3670 - val_accuracy: 0.8542\n",
      "Epoch 7/15\n",
      "57/57 [==============================] - 37s 655ms/step - loss: 0.1955 - accuracy: 0.9152 - val_loss: 0.3801 - val_accuracy: 0.8544\n",
      "Epoch 8/15\n",
      "57/57 [==============================] - 37s 653ms/step - loss: 0.1839 - accuracy: 0.9197 - val_loss: 0.3303 - val_accuracy: 0.8739\n",
      "Epoch 9/15\n",
      "57/57 [==============================] - 37s 654ms/step - loss: 0.1709 - accuracy: 0.9245 - val_loss: 0.3565 - val_accuracy: 0.8779\n",
      "Epoch 10/15\n",
      "57/57 [==============================] - 37s 654ms/step - loss: 0.1573 - accuracy: 0.9301 - val_loss: 0.3503 - val_accuracy: 0.8649\n",
      "Epoch 11/15\n",
      "57/57 [==============================] - 37s 654ms/step - loss: 0.1508 - accuracy: 0.9325 - val_loss: 0.2968 - val_accuracy: 0.8970\n",
      "Epoch 12/15\n",
      "57/57 [==============================] - 37s 654ms/step - loss: 0.1422 - accuracy: 0.9360 - val_loss: 0.3050 - val_accuracy: 0.8963\n",
      "Epoch 13/15\n",
      "57/57 [==============================] - 37s 654ms/step - loss: 0.1334 - accuracy: 0.9395 - val_loss: 0.3484 - val_accuracy: 0.8896\n",
      "Epoch 14/15\n",
      "57/57 [==============================] - 37s 654ms/step - loss: 0.1285 - accuracy: 0.9414 - val_loss: 0.3365 - val_accuracy: 0.8844\n",
      "Epoch 15/15\n",
      "57/57 [==============================] - 37s 656ms/step - loss: 0.1191 - accuracy: 0.9452 - val_loss: 0.4151 - val_accuracy: 0.8850\n"
     ]
    }
   ],
   "source": [
    "#Run model \n",
    "unet = DenseUnet()\n",
    "unet.train(train_dataset,epochs=15,steps_per_epoch=STEPS_PER_EPOCH,\n",
    "           validation_steps=VALIDATION_STEPS,\n",
    "           validation_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f663ec46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f663ec46",
    "outputId": "e3844c40-996f-45c6-9fe5-da17b9e6da9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 23s 394ms/step - loss: 0.4081 - accuracy: 0.8864\n",
      "Accuracy: 88.64%\n"
     ]
    }
   ],
   "source": [
    "#Model Evaluation\n",
    "unet.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53e752d",
   "metadata": {
    "id": "c53e752d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "TL_Unet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
